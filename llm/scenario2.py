import transformers
import argparse
import tqdm
import re
import torch
import numpy as np

from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import AutoConfig
from colorama import Fore, Style
from transformers import AutoTokenizer
from transformers import pipeline

def get_prompt(n, bernoulli_p):

    txt1 = "Consider the following sequence of outcomes generated by two Bernoulli distributions, where all even outcomes are generated by a Bernoulli distribution with parameter 'p' and odd outcomes are generated from a Bernoulli distribution with parameter '1-p'\n\n"

    generate_coinflips = []
    for i in range(n):
        if i % 2 == 0:
            coinflip = "1" if np.random.uniform(0, 1) < bernoulli_p else "0"
        else:
            coinflip = "0" if np.random.uniform(0, 1) < bernoulli_p else "1"
        generate_coinflips.append(coinflip)

    generate_coinflips = "".join(generate_coinflips)

    txt2 = "\n\n The next 20 most likely sequence of outcomes are:\n\n"

    prompt = txt1 + generate_coinflips + txt2

    return prompt


def gen_txt(model_id, bernoulli_p):

    device = "cuda:0"

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                device_map=device,
            )

    torch.compile(model, mode="reduce-overhead", fullgraph=True)

    np.random.seed(0)
    seeds = 100
    all_errs = []
    for n in tqdm.tqdm(range(2, 100, 6)):
        cur_errs = []
        for seed in range(seeds):
            prompt = get_prompt(n, bernoulli_p)

            inp_tokens = tokenizer([prompt], return_tensors="pt").to(device)
            ln = inp_tokens['input_ids'].shape[1]

            generated_ids = model.generate(
                **inp_tokens,
                do_sample=False,
                top_p=None,
                temperature=None,
                use_cache=True, # Use-kv cache
                max_new_tokens=20)[:, ln:]

            gen_tokens = tokenizer.batch_decode(generated_ids)[0]
            gen_tokens = gen_tokens.replace('\n', '\n\n')

            # get all numbers from the generated text
            numbers = re.findall(r'\d+', gen_tokens)

            maxidx = -1
            maxval = -1
            for idx, ne in enumerate(numbers):
                if len(ne) > maxval:
                    maxval = max(maxval, len(ne))
                    maxidx = idx

            errs = []
            for idx, ni in enumerate(numbers[maxidx]):
                if ni == "1" or ni == "0":
                    if ni == "1":
                        err = 1 - bernoulli_p
                    else:
                        err = bernoulli_p

                    if idx % 2 == 1:
                        err = 1 - err
                    errs.append(err)

            if len(errs) == 0:
                errs = [all_errs[-1][1]]
                print("flag", gen_tokens, numbers[maxidx])
            cur_errs.append((n, np.mean(errs)))
        all_errs.append(cur_errs)

    # store all_errs
    mname = model_id.split("/")[1]
    b_txt = str(int(bernoulli_p * 100))
    np.save("data/scenario2_%s_%s.npy" % (mname, b_txt), all_errs)

    all_errs = np.array(all_errs)
    mean = all_errs.mean(1)
    std = all_errs.std(1)

    print("-------------------------------------------")
    print(Style.DIM + prompt + Style.NORMAL + gen_tokens)



if __name__ == "__main__":

    # argparse command line for midx and bernoulli_p
    parser = argparse.ArgumentParser()
    parser.add_argument("--m", type=int, default=2)
    parser.add_argument("--p", type=float, default=0.9)

    args = parser.parse_args()

    midx = args.m
    bernoulli_p = args.p


    model_names = ["tiiuae/falcon-7b",
                   "mistralai/Mistral-7B-v0.1",
                   "meta-llama/Llama-2-7b-hf",
                   "google/gemma-7b",
                  ]

    mname = model_names[midx]
    print("Model: %s" % mname)

    gen_txt(mname, bernoulli_p)

